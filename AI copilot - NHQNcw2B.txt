from flask import Flask, request, jsonify, send_from_directory, render_template
import requests
import json
import subprocess
import os
import re

app = Flask(__name__)

# Dossier pour stocker les fichiers générés
GENERATED_FILES_DIR = "generated_files"
os.makedirs(GENERATED_FILES_DIR, exist_ok=True)

# URLs et clés des APIs
OLLAMA_API_URL = "http://localhost:11434/api/chat"
MISTRAL_API_URL = "https://api.mistral.ai/v1/chat/completions"
MISTRAL_API_KEY = "JCASwm1qYdbfSCd7ilpEgUU0oRv7LcbG"

# Modèles disponibles
OLLAMA_MODELS = []
MISTRAL_MODELS = [
    "pixtral-large-latest",
    "pixtral-medium-latest",
    "pixtral-small-latest",
    "mistral-large-latest",
    "ministral-3b-latest",
    "ministral-8b-latest",
    "mistral-small-latest",
    "codestral-latest",
    "mistral-medium",
    "mistral-embed",
    "mistral-moderation-latest",
    "pixtral-12b-2409",
    "open-mistral-nemo",
    "open-codestral-mamba",
    "open-mistral-7b",
    "open-mixtral-8x7b",
    "open-mixtral-8x22b",
    
]

# Stocker l'historique et la mémoire contextuelle
conversation_history = []
contextual_memory = []

# Route pour servir la page HTML
@app.route('/')
def serve_index():
    return render_template("index.html")

# Route pour interagir avec les APIs Ollama et Mistral
@app.route('/ask', methods=['POST'])
def ask_ai():
    global conversation_history, contextual_memory

    user_message = request.json.get("message", "")
    selected_model = request.json.get("model", "llama3.2")  # Par défaut Ollama

    # Ajouter le message utilisateur à l'historique
    conversation_history.append({"role": "user", "content": user_message})

    # Vérifier si le modèle appartient à Ollama ou Mistral
    if selected_model in OLLAMA_MODELS:
        ai_response, memory_update = query_ollama(selected_model)
    elif selected_model in MISTRAL_MODELS:
        ai_response, memory_update = query_mistral(selected_model, user_message)
    else:
        return jsonify({"error": "Modèle non reconnu"}), 400

    # Ajouter la réponse de l'IA à l'historique
    conversation_history.append({"role": "assistant", "content": ai_response})

    # Si l'IA identifie des informations à mémoriser, les stocker
    if memory_update:
        contextual_memory.append(memory_update)

    # Gérer la création de fichiers si du code est généré
    create_files_from_response(ai_response)

    return jsonify({"response": ai_response})

# Route pour lister les modèles disponibles
@app.route('/models', methods=['GET'])
def list_models():
    global OLLAMA_MODELS
    try:
        # Récupérer les modèles locaux d’Ollama
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
        if result.returncode == 0:
            OLLAMA_MODELS = [line.split()[0] for line in result.stdout.splitlines() if line]
    except Exception as e:
        OLLAMA_MODELS = []

    # Fusionner les modèles Ollama et Mistral
    models = {"ollama": OLLAMA_MODELS, "mistral": MISTRAL_MODELS}
    return jsonify(models)

# Route pour explorer les fichiers générés
@app.route('/explore', methods=['GET'])
def explore_files():
    files_structure = {}
    for root, dirs, files in os.walk(GENERATED_FILES_DIR):
        path = root.split(os.sep)
        current_dir = files_structure
        for folder in path[1:]:
            current_dir = current_dir.setdefault(folder, {})
        for file in files:
            current_dir[file] = os.path.join(root, file)
    return jsonify(files_structure)

# Route pour lire un fichier
@app.route('/file', methods=['GET'])
def read_file():
    filepath = request.args.get("path")
    if not filepath or not os.path.exists(filepath):
        return jsonify({"error": "Fichier introuvable"}), 404
    with open(filepath, "r", encoding="utf-8") as f:
        return jsonify({"content": f.read()})

# Route pour modifier un fichier
@app.route('/file', methods=['POST'])
def edit_file():
    data = request.json
    filepath = data.get("path")
    content = data.get("content")
    if not filepath or not os.path.exists(filepath):
        return jsonify({"error": "Fichier introuvable"}), 404
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(content)
    return jsonify({"message": "Fichier mis à jour avec succès"})

# Fonction pour interroger l'API Ollama
def query_ollama(model):
    global contextual_memory

    full_context = contextual_memory + conversation_history
    payload = {
        "model": model,
        "messages": full_context,
        "system_prompt": (
            "Tu es une IA conversationnelle spécialisée dans l'aide à la programmation. "
            "Respecte les consignes et génère des fichiers si nécessaire."
        ),
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        if response.status_code != 200:
            return f"Erreur de l'API Ollama : {response.status_code}", None

        full_response = ""
        memory_update = None

        for line in response.iter_lines():
            if line:
                json_part = json.loads(line.decode('utf-8'))
                if "memory_update" in json_part:
                    memory_update = json_part["memory_update"]
                full_response += json_part.get("message", {}).get("content", "")

        return full_response or "Pas de réponse reçue.", memory_update
    except Exception as e:
        return f"Erreur : {str(e)}", None

# Fonction pour interroger l'API Mistral
def query_mistral(model, user_message):
    headers = {"Authorization": f"Bearer {MISTRAL_API_KEY}", "Content-Type": "application/json"}
    payload = {
        "model": model,
        "messages": [{"role": "user", "content": user_message}],
    }

    try:
        response = requests.post(MISTRAL_API_URL, json=payload, headers=headers)
        if response.status_code == 200:
            json_response = response.json()
            ai_response = json_response.get("choices", [{}])[0].get("message", {}).get("content", "")
            return ai_response, None
        else:
            return f"Erreur de l'API Mistral : {response.status_code}", None
    except Exception as e:
        return f"Erreur : {str(e)}", None

# Fonction pour créer des fichiers à partir d'une réponse
def create_files_from_response(response):
    code_blocks = re.findall(r"(?:\/\/\s*file:\s*([\w.\-]+))?\s*```(\w+)?\n(.*?)```", response, re.DOTALL)
    for i, (filename_hint, language, code) in enumerate(code_blocks):
        extension = {
            "html": "html",
            "css": "css",
            "js": "js",
            "python": "py",
            "java": "java",
            "php": "php",
            "json": "json",
            "xml": "xml",
            "txt": "txt",
        }.get(language.lower() if language else "txt", "txt")

        filename = filename_hint or f"file_{i + 1}.{extension}"
        filepath = os.path.join(GENERATED_FILES_DIR, filename)
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(code.strip())
        print(f"Fichier créé : {filepath}")

if __name__ == "__main__":
    app.run(debug=True)
